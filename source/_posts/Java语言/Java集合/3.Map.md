# HashMap

## 概览

### 继承体系

![image-20210601175023607](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210601175023607.png)

+ HashMap：不保证遍历顺序，非线程安全
+ LinkedHashMap：HashMap的子类，保持了记录插入顺序，遍历会得到先插入的Node，也可以按照访问顺序排列，实现LRU算法
+ TreeMap：默认按照key升序排列
+ Hashtable：线程安全，效率低，所有方法加synchronized修饰，需要线程安全场景用ConcurrentHashMap即可

### 数据结构

数组+链表+红黑树

![image-20210601212902002](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210601212902002.png)

```java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;
}

Node<K,V>[] table; // 第一次put时初始化，length总是2的幂次倍
int size; // HashMap中实际存在的键值对数量
int modCount; // 记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。
int threshold; // 所能容纳的最大Node(键值对)个数。threshold = length * Load factor,超过这个数目就重新resize(扩容)
final float loadFactor; 
transient Set<Map.Entry<K， V>> entrySet;

static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // 默认初始容量
static final int MAXIMUM_CAPACITY = 1 << 30; // 最大数组容量
static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认负载因子为0.75
static final int TREEIFY_THRESHOLD = 8;  // 单个Node下的值的个数大于8时候，会将链表转换成为红黑树。
static final int UNTREEIFY_THRESHOLD = 6; // 单个Node节点下的红黑树节点的个数小于6时候，会将红黑树转化成为链表。
static final int MIN_TREEIFY_CAPACITY = 64; // 树化的最小数组容量
```

### 特性

+ 运行key为null（hash为null），存放在entry数组的第0个位置上

+ 如果put操作中，只改变了value，则modCount不变

+ 要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了（因为put和get都需要靠hashCode找到他的索引下标）。

+ 当同一个索引位置的节点在增加后达到8个时，并且此时数组的长度大于等于 64，则则会触发链表节点（Node）转红黑树节点（TreeNode），转成红黑树节点后，其实链表的结构还存在，通过 next 属性维持。

    当单个桶中元素数量小于6时，进行反树化

+ HashMap查找添加元素的时间复杂度都为O(1)。数组的查询效率为O(1)，链表的查询效率是O(k)，红黑树的查询效率是O(log k)，k为桶中的元素个数

+ 为它的子类LinkedHashMap提供一些钩子方法

## 基本流程

### put()

1. 计算key对应的hashCode
2. 在entry数组较小时也能让高位也参与hash的运算：`hash = (h = key.hashCode()) ^ (h >>> 16)`
3. 如果table不存在，则先初始化
4. 如果索引`i = (n - 1) & hash`的位置为null，则直接新建节点
5. 看table[i]首节点是否和key相同，相同则覆盖value，返回oldValue，判断是否相同的方法：hash相同，并且equals方法也相同
6. 如果是红黑树节点，调用红黑树节点的插入方法
7. 遍历下拉链表，如果找到了value就覆盖，找到链表尾部就新建Node，并且查看是否需要进行树化
8. 插入成功后查看是否需要扩容、modCount++、提供钩子方法等

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

// 核心部分
final V putVal(int hash, K key, V value) {
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null); // 尾插法
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

**注意事项**

+ 当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率。

### get()

+ 获取对应hash
+ 看table是否存在，以及`tab[(n - 1) & hash]`是否存在
+ 检查索引下的第一个节点的hash、equals是否相同
+ 如果是红黑树节点，调用红黑树节点方法搜索
+ 在下拉链表中搜索

```java
public V get(Object key) {
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}
final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
            (first = tab[(n - 1) & hash]) != null) {
        if (first.hash == hash && 
                ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        if ((e = first.next) != null) {
            if (first instanceof TreeNode)
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            do {
                if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

**注意事项**

+ 两种情况会返回null：value不存在、put的value是null

### resize()

如果没有初始化，则首先执行初始化，否则执行正常的扩容操作。将容量变成原来的两倍

遍历原Entry数组，把所有的Entry重新Hash到新数组

```java
final Node<K, V>[] resize() {
    Node<K, V>[] newTab = (Node<K, V>[]) new Node[newCap];
    for (int j = 0; j < oldCap; ++j) {
        Node<K, V> e;
        if ((e = oldTab[j]) != null)
        {
            oldTab[j] = null; // 将老表的节点设置为空, 以便垃圾收集器回收空间
            if (e.next == null) // 只有一个节点,直接散列
                newTab[e.hash & (newCap - 1)] = e;
            else if (e instanceof TreeNode) // 调用红黑树的重新散列
                ((TreeNode<K, V>) e).split(this, newTab, j, oldCap);
            else {
                Node<K, V> loHead = null, loTail = null;
                Node<K, V> hiHead = null, hiTail = null;
                Node<K, V> next;
                do {
                    next = e.next;
                    if ((e.hash & oldCap) == 0) { // 分成两条链表，使用尾插法
                        if (loTail == null)
                            loHead = e;
                        else
                            loTail.next = e;
                        loTail = e;
                    } else {
                        if (hiTail == null)
                            hiHead = e;
                        else
                            hiTail.next = e;
                        hiTail = e;
                    }
                } while ((e = next) != null);
                if (loTail != null) {
                    loTail.next = null;
                    newTab[j] = loHead;
                }
                if (hiTail != null) {
                    hiTail.next = null;
                    newTab[j + oldCap] = hiHead;
                }
            }
        }
    }
    return newTab;
}
```

例如00011,10011，如果oldCap=10000(16)
00011 & 10000 = 0
10011 & 10000 = 10000 != 0
将原本在一个索引的节点分成两条链表，一个放在原位置处，一个放在[原位置 + oldCap]处



### 初始化

第一次put时才真正创建数组，分配空间

**tableSizeFor（）**

得到第一个大于等于cap的二的幂次方

用于从构造函数中获取用户输入，并且保证了 table 数组的长度总是 2 的次幂

无符号右移：就是右移之后，无论该数为正还是为负，右移之后左边都是补上0

```java
public HashMap(int initialCapacity, float loadFactor) {
    this.loadFactor = loadFactor;
    this.threshold = tableSizeFor(initialCapacity);
}
static final int tableSizeFor(int cap) {
    int n = cap - 1;
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

例如10001(17)
首先cap-1:     10000
n |= n >>> 1: n=(10000 | 01000) =11000
n |= n >>> 2: n=(11000 | 00110) =11110
n |= n >>> 4: n=(11110 | 00001) =11111
n |= n >>> 8: n=(11111 | 00000) =11111
n |= n >>> 16: n=(11111 | 00000) =11111
return n+1:100000(32)
...
因为int为32位,所以为了保证结果,最后右移16位结束
使得要求的数字1开始的后面全为1

### 迭代器HashIterator

+ 初始化记录modCount当前值，并让next指向第一个非空元素，而current=null
+ nextNode方法，首先查看modCount值以决定是否fail-fast
+ 然后检查当前元素是否为空，为空则抛出NoSuchElementException异常
+ 最后让current等于当前元素，next继续指向下一个非空元素
+ hasNext方法则简单判断next是否为空即可
+ remove方法首先判断当前节点是否为空，为空则抛出IllegalStateException异常
+ 随后查看modCount值决定是否fail-fast
+ 删除该节点，且修改expectedModCount，即迭代器中的remove方法不会导致fail-fast

```java
abstract class HashIterator
{
    Node<K， V> next;        
    Node<K， V> current;     
    int expectedModCount;  // for fast-fail
    int index;             

    HashIterator()
    {
        expectedModCount = modCount;
        Node<K， V>[] t = table;
        current = next = null;
        index = 0;
        if (t != null && size > 0)
        { 
            do
            {
            } while (index < t.length && (next = t[index++]) == null);
            // 让next能指向第一个非空的元素
        }
    }

    public final boolean hasNext()
    {
        return next != null;
    }

    final Node<K， V> nextNode()
    {
        Node<K， V>[] t;
        Node<K， V> e = next;
        if (modCount != expectedModCount) // fail-fast机制
            throw new ConcurrentModificationException();
        if (e == null)
            throw new NoSuchElementException();
        //找到下一个非空元素
        if ((next = (current = e).next) == null && (t = table) != null)
        {
            do
            {
            } while (index < t.length && (next = t[index++]) == null);
        }
        return e;
    }

    public final void remove()
    {
        Node<K， V> p = current;
        if (p == null)
            throw new IllegalStateException();
        if (modCount != expectedModCount) // 依然是fail-fast
            throw new ConcurrentModificationException();
        current = null;
        K key = p.key;
        removeNode(hash(key)， key， null， false， false);
        expectedModCount = modCount; // 此处remove不会触发fail-fast
    }
}
```

## 扩展

### JDK1.7和JDK1.8中HashMap有什么区别？

+ 数据结构上：JDK1.8中当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。
+ 插入数据方式上：
    + JDK1.7使用头插法插入元素，在多线程的环境下有可能导致环形链表的出现，扩容的时候会导致死循环（导致死循环的主要原因是扩容后，节点的顺序会反掉）。
    + JDK1.8使用尾插法插入元素（直接插入到链表尾部/红黑树），解决了多线程死循环问题，但仍是非线程安全的，多线程时可能会造成数据丢失问题。

### HashMap线程安全问题

例如两个线程同时调用put操作，他们同时进入到了if语句后，先写入的Node会被后写入的Node覆盖掉

```java
if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
```

### 为什么不直接用红黑树？

红黑树节点的大小是普通节点的两倍、红黑树需要进行左旋，右旋，变色这些操作来保持平衡，新增节点的效率低，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度。

**为什么链表改为红黑树的阈值是 8?**

理想情况下使用随机的哈希码，容器中节点分布在 hash 桶中的频率遵循**泊松分布**，按照泊松分布的计算公式计算出桶中元素个数和概率的对照表，链表中元素个数为 8 时的概率非常小。

**默认加载因子为什么是 0.75？**

作为一般规则，默认负载因子（0.75）在时间和空间成本上提供了很好的折衷，是对空间和时间效率的一个平衡选择。较高的值会降低空间开销，但提高查找成本。较低的值浪费空间。

### HashTable与HashMap的区别

+ 比HashMap多了个线程安全，直接在方法上锁，并发度很低，最多同时允许一个线程访问，效率比较低下

+ 如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。也就是说对于Hashtable而言，synchronized是针对整张Hash表的，即每次锁住整张表让线程独占。相当于所有线程进行读写时都去竞争一把锁，导致效率非常低下。

+ Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null。Hashtable在我们put 空值的时候会直接抛空指针异常，但是HashMap却做了特殊处理。

+ 初始化容量不同：HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75。

+ HashMap 的 hash 值重新计算过，Hashtable 直接使用 hashCode。

    ```java
    int hash = key.hashCode();
    int index = (hash & 0x7FFFFFFF) % tab.length;
    ```

+ 迭代器不同：HashMap 中的 Iterator 迭代器是 fail-fast 的，⽽ Hashtable 的 Enumerator是安全失败机制（fail-safe），这种机制会使你此次读到的数据不⼀定是最新的数据。

### JDK1.7死循环

+ 扩容操作中，把旧表中所有Node重新计算索引下标并散列到新表对应索引处(采用头插法)
+ 把新表的引用赋值给table



首先会导致链表反转：

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210601204630630.png" alt="image-20210601204630630" style="zoom: 67%;" />

如果线程1、线程2同时操作，线程2执行到e = A、next = B时切换到线程1，线程1执行完resize导致链表反转成C->B->A->null。

线程2继续执行以下代码，则产生了死循环：

```
e.next = newTable[i];
newTable[i] = e;
e = next;
```

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210601205841811.png" alt="image-20210601205841811" style="zoom:67%;" />

```java
void resize(int newCapacity)
{
    Entry[] oldTable = table;
    Entry[] newTable = new Entry[newCapacity];
    transfer(newTable);
    table = newTable;
}

void transfer(Entry[] newTable)
{
    for (Entry<K, V> e : table)  // 遍历旧表中每个Entry
    {
        while (null != e) // 对不为空的(链表)进行操作
        {
            Entry<K, V> next = e.next;
            e.hash = null == e.key ? 0 : hash(e.key);
            int i = indexFor(e.hash, newCapacity); // 通过新的容量大小散列得到新的索引
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

### **hashCode与equals**

**equals**

在未重写equals方法我们是继承了Object的equals方法，默认比较两个对象的内存地址

+ 对于值对象，==比较的是两个对象的值
+ 对于引用对象，==比较的是两个对象的地址

```java
// 默认情况是:
public boolean equals(Object obj) {
    return (this == obj);
}
// 一般会重写equals方法以比较两个对象的内容
public boolean equals(Object obj){
    if(this==obj) return true;
    if(obj==null) return false;
    if(getClass()!=obj.getClass()) return false;

    Person person = (Person) obj;
    return Objects.equals(name， person.name) &&Objects.equals(age， person.age);
}
所以基本数据类型用==判断相等，引用数据类型都用equals()进行判断(String也是引用类型)
即==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同
==指引用是否相同 equals()指的是值是否相同
```

**hashcode**

```java
因为HashSet或HashMap等集合中判断元素是否相等用到了hashcode是否相等，所以为避免我们认为相等但是逻辑判断却不相等的情况出现，自定义类重写equals必须重写hashcode方法
public native int hashCode();

无论何时覆盖该方法，通常需要覆盖`hashCode`方法，以便维护`hashCode`方法的通用合同，该方法规定相等的对象必须具有相等的哈希码。

@Override
public int hashCode()
{
    return Objects.hash(username， age);
}
```

+ 如果两个对象相等，则hashcode一定也是相同的

+ 两个对象相等，对两个equals方法返回true

+ 两个对象有相同的hashcode值，它们也不一定是相等的

+ hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。

**例子**

使用Set存放学生信息，此时不小心对同一个人录入了两次（学号和姓名相同，但不是同一个对象）（我们逻辑上认为是同一个人，并且已重写了equals()方法，但是没有重写hashcode()方法）。这时Set中按逻辑应该只有一个学生，但是实际上却有两个

这是因为我们没有重写父类（Object）的hashcode方法，Object的hashcode方法会根据两个对象的地址生成对相应的hashcode；s1和s2是分别new出来的，那么他们的地址肯定是不一样的，自然hashcode值也会不一样。

Set区别对象的标准是，两个对象hashcode是不是一样，再判定两个对象是否equals；此时因为hashCode不一样，所以Set判定他们不是同一个对象，在Set中该学生信息就出现了两次

# ConcurrentHashMap

## 数据结构

```java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;
}

// 在扩容时发挥作用，hash值为MOVED(-1)，存储nextTable的引用
final class ForwardingNode<K,V> extends Node<K,V> {
    final Node<K,V>[] nextTable;
    ForwardingNode(Node<K,V>[] tab) {
        super(MOVED, null, null, null);
        this.nextTable = tab;
    }
}

volatile Node<K,V>[] table;
volatile Node<K,V>[] nextTable; // 默认为null，扩容时新生成的数组，其大小为原数组的两倍。
volatile int sizeCtl; // 控制初始化和扩容的


static final int MOVED     = -1;  // 表示正在扩容
```

+ Node节点设置了volatile关键字修饰，致使它每次获取的都是**最新**设置的值
+ 抛弃了Segment分段锁机制，利用CAS+Synchronized来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构。
+ 在构造函数中只会初始化sizeCtl值，并不会直接初始化table，而是延缓到第一次put操作。
+ ConcurrentHashMap不允许key或value为null值



- 在集合**新建而未初始化**时，sizeCtl用于记录初始容量大小
- 在集合**初始化过程中**，sizeCtl值设置为 -1 表示集合正在初始化中，其他线程发现该值为 -1 时会让出CPU资源以便初始化操作尽快完成 。
- 集合**初始化完成后**，sizeCtl 用于记录当前集合的负载容量值，也就是触发集合扩容的极限值 。
- 集合**正在扩容时**，sizeCtl 用于记录当前扩容的并发线程数情况，该状态下 sizeCtl < 0 。

## 常用方法

### put

ConcurrentHashMap添加数据时，采取了CAS+synchronize结合策略。首先会判断该节点是否为null，如果为null，尝试使用CAS添加节点；如果添加失败，说明发生了并发冲突，再对节点进行上锁并插入数据。在并发较低的情景下无需加锁，可以显著提高性能。同时只会CAS尝试一次，也不会造成线程长时间等待浪费cpu时间的情况。

```java
final V putVal(K key, V value) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;

    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null, new Node<K,V>(hash, key, value, null)))
                break;                   
        }
        else if ((fh = f.hash) == MOVED) 
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            synchronized (f) { // f是要插入的索引下标上的首节点
                if (tabAt(tab, i) == f) {
                    if (fh >= 0) {
                        // 尾查法往链表添加节点
                    }
                    else if (f instanceof TreeBin) {
                        // 调用红黑树添加节点方法
                    }
                }
            }
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}

static final int spread(int h) {
    return (h ^ (h >>> 16)) & HASH_BITS; // 0x7fffff
}
```

+ 判断键值是否为`null`，为`null`抛出异常。
+ 调用`spread()`方法计算key的hashCode()获得哈希地址
+ 如果当前table为空，则初始化table，需要注意的是这里并没有加`synchronized`，也就是允许多个线程去**尝试**初始化table，但是在初始化函数里面使用了`CAS`保证只有一个线程去执行初始化过程。
+ 使用`i = (n - 1) & hash`计算出待插入键值的下标，如果该下标上的bucket为`null`，则直接调用实现`CAS`原子性操作的`casTabAt()`方法将节点插入到table中，如果插入成功则完成put操作，结束返回。插入失败(被别的线程抢先插入了)则继续往下执行。
+ 如果该下标上的节点(头节点)的哈希地址为MOVED(-1)，说明当前f是ForwardingNode节点，意味有其它线程正在扩容，该线程执行`helpTransfer()`方法协助扩容。
+ 如果该下标上的bucket不为空，且又不需要扩容，则进入到bucket中，同时**使用synchroized锁住这个bucket**，注意只是锁住该下标上的bucket而已，其他的bucket并未加锁，其他线程仍然可以操作其他未上锁的bucket
+ 进入到bucket里面，首先判断这个bucket存储的是红黑树还是链表。
+ 如果是**链表**，则遍历链表看看是否有hash和equals相同的节点，有的话则根据传入的参数进行覆盖或者不覆盖，没有找到相同的节点的话则将新增的节点**插入到链表尾部**。如果是**红黑树**，则将节点插入。到这里**解锁**。
+ 最后判断该bucket上的链表长度是否大于**链表转红黑树的阈值(8)**，大于则将链表转成红黑树。
+ 调用`addCount()`方法，将键值对数量+1，并检查是否需要扩容。

### get

```java
public V get(Object key) {
    int h = spread(key.hashCode());
    if ((tab = table) != null && (n = tab.length) > 0 &&
            (e = tabAt(tab, (n - 1) & h)) != null) {
        if ((eh = e.hash) == h) { // 头节点寻找
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        else if (eh < 0) 
            // 红黑树中寻找
        while ((e = e.next) != null) { 
            // 链表中寻找
        }
    }
    return null;
}
```

1. 调用`spread()`方法计算key的hashCode()获得哈希地址。
2. 如果table不为空，对应key所在bucket不为空：`tabAt(tab, (n - 1) & h))`，则到bucket中查找。
3. 如果头节点hash、equals相同，则返回头节点值
4. 如果bucket的头节点的hash小于0，则代表这个bucket存储的是红黑树，则在红黑树中查找。
5. 如果bucket头节点的哈希地址不小于0，则代表bucket为链表，遍历链表，找到则返回该键key的值，找不到则返回null。

### remove

1. 调用`spread()`方法计算出键key的哈希地址。
2. 计算出键key所在的数组下标，如果table为空或者bucket为空，则返回`null`。
3. 判断当前table是否正在扩容，如果在扩容则调用helpTransfer方法协助扩容。
4. 如果table和bucket都不为空，table也不处于在扩容状态，则**锁住当前bucket**，对bucket进行操作。
5. 根据bucket的头结点判断bucket是链表还是红黑树。
6. 在链表或者红黑树中移除哈希地址、键key相同的节点。
7. 调用`addCount`方法，将当前table存储的键值对数量-1。

### 初始化

```java
public ConcurrentHashMap(int initialCapacity) {
    int cap = tableSizeFor(initialCapacity);
    this.sizeCtl = cap;
}

// 第一次put时才调用
private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {
        if ((sc = sizeCtl) < 0)
            Thread.yield(); 
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
            if ((tab = table) == null || tab.length == 0) {
                table = (Node<K,V>[])new Node<?,?>[n];
                sizeCtl = n - (n >>> 2);
            }
            break;
        }
    }
    return tab;
}
```

+ table的初始化只能由一个线程完成，但是每个线程都可以争抢去初始化table。
+ 首先，判断table是否为null，即需不需要首次初始化，如果某个线程进到这个方法后，其他线程已经将table初始化好了，那么该线程结束该方法返回。
+ 如果table为null，进入到while循环，如果`sizeCtl`小于0(其他线程正在对table初始化)，那么该线程调用`Thread.yield()`挂起该线程，让出CPU时间，该线程也从运行态转成就绪态，等该线程从就绪态转成运行态的时候，别的线程已经table初始化好了，那么该线程结束while循环，结束初始化方法返回。如果从就绪态转成运行态后，table仍然为`null`，则继续while循环。
+ 如果table为null且`sizeCtl`不小于0，则调用实现`CAS`原子性操作的`compareAndSwap()`方法将sizeCtl设置成-1，告诉别的线程我正在初始化table，这样别的线程无法对table进行初始化。如果设置成功，则初始化table，容量大小为默认的容量大小(16)，或者为sizeCtl。其中sizeCtl的初始化是在构造函数中进行的。并设置sizeCtl的值为数组长度的3/4（`threshold`的作用），当ConcurrentHashMap储存的键值对数量大于这个阈值，就会发生扩容。

### 扩容

**触发扩容**

1. 添加新元素后，元素个数达到扩容阈值触发扩容。
2. 调用 putAll 方法，发现容量不足以容纳所有元素时候触发扩容。

3. 某个槽内的链表长度达到 8，但是数组长度小于 64 时候触发扩容。

**扩容操作**

- 构建一个nextTable，大小为table的两倍。这个过程只能只有单个线程进行nextTable的初始化（通过Unsafe.compareAndSwapInt修改sizeCtl值，保证只有一个线程能够初始化nextTable）
- 将原来table里面的内容复制到nextTable中，这个步骤是允许**多线程**操作的，所以性能得到提升，减少了扩容的时间消耗。

**扩容时其他操作**

扩容状态下其他线程对集合进行插入、修改、删除等操作时遇到 ForwardingNode 节点会调用helpTransfer方法帮助扩容

## 扩展

### JDK1.7的ConcurrentHashMap

+ 基本思想是将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。
+ Segment 是一个ReentrantLock，每一个Segment元素存储的是HashEntry数组+链表，每一个Segment其实就相当于一个HashMap
+ put操作需要加锁，get操作不用加锁（通过使用volatile和巧妙的操作保证同步）
+ ConcurrentHashMap定位一个元素的过程需要进行两次Hash操作，第一次Hash定位到Segment，第二次Hash定位到元素所在的链表的头部

![20216212](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/20216212.png)

```java
final Segment<K,V>[] segments;

static final class Segment<K,V> extends ReentrantLock {
	volatile HashEntry<K,V>[] table;
	int count;
	int modCount;
	int threshold;
	float loadFactor;
}
static final class HashEntry<K,V> { 
    final K key; 
    final int hash; 
    volatile V value; 
    final HashEntry<K,V> next; 
}
```

### ConcurrentHashMap不允许key或value为null值

put操作中有相应的判断：

```java
if (key == null || value == null) throw new NullPointerException();
```

但这样设计的原因是避免二义性：假定ConcurrentHashMap也可以存放value为null的值。那调用map.get(key)时如果返回了null，有两重含义:

**1.这个key从来没有在map中映射过。**

**2.这个key的value在设置的时候，就是null。**

对于HashMap来说，它的正确使用场景是在单线程下使用。所以在单线程中，当我们得到的value是null的时候，可以用hashMap.containsKey(key)方法来区分上面说的两重含义。

而ConcurrentHashMap的使用场景为多线程。假设concurrentHashMap允许存放值为null的value。这时有A、B两个线程。线程A调用concurrentHashMap.get(key)方法，返回为null，我们还是不知道这个null是没有映射的null还是存的值就是null。

假设此时返回null的真实情况是因为这个key没有在map里面映射过。用concurrentHashMap.containsKey(key)来验证假设是否成立，期望的结果是返回false。

但是在我们调用concurrentHashMap.get(key)方法之后，containsKey方法之前，有一个线程B执行了concurrentHashMap.put(key,null)的操作。那么我们调用containsKey方法返回的就是true了。这就与我们的假设的真实情况不符合了。	

# Set

## HashSet

+ 不保证集合顺序
+ 允许null值
+ HashSet 不允许重复的值

```java
//底层使用HashMap来保存HashSet中所有元素
private transient HashMap<E，Object> map;

//定义一个虚拟的Object对象作为HashMap的value
private static final Object PRESENT = new Object();

public boolean add(E e) {

    //因为put会返回原Value，如果放入相同的，则返回就不为null

    //则该方法返回false，代表添加不成功，代表已经有了元素
	//HashSet的值存放于HashMap的key上，HashMap的value统一为PRESENT
    return map.put(e， PRESENT)==null;

}
```

## LinkedHashSet

+ LinkedHashSet 内部是通过 LinkedHashMap 来实现的。
+ 继承HashSet
+ LinkedHashSet的构造器调用父类HashSet的其中一个构造器，将map初始化为LinkedHashMap，所以再调用方法时，就会有LinkedHashMap的调用效果
+ 用双向链表维持元素的插入顺序
+ 重新插入时不会影响顺序