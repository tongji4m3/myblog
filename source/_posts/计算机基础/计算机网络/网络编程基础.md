# 基本套接字编程

## TCP原理

### 三次握手

1. 服务器必须准备好接受外来的连接。通常是通过调用socket、bind和listen三个函数完成，此时为**被动打开**
2. 客户通过调用connect发起**主动打开**，这导致客户端TCP发送一个SYN，告诉服务器自己的初始序列号（不带数据）。
3. 服务器必须确认（ACK）客户的SYN，同时自己也要发送一个SYN，含有服务器将在同一连接中发送数据的初始序列号。
4. 客户必须确认服务器的SYN

![image-20210603165512924](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603165512924.png)

### 四次挥手

1. 某个应用进程（假设是客户端）首先调用close，执行**主动关闭**，于是发送一个FIN，表示数据发送完毕
2. 接收到这个FIN的对端（假设是服务器）执行**被动关闭**，给客户发送回ACK。FIN的接收意味着服务器端在相应连接上再无额外数据可接收
3. 一段时间后（这段时间内，服务器端仍然可以向客户端发送数据，称为半关闭状态），服务器端也将调用close关闭他的套接字，它会像客户端也发送一个FIN
4. 执行主动关闭的那端会发送ACK确认这个FIN

![image-20210603165546769](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603165546769.png)

### TCP状态转换图

用粗实线表示客户状态转换，粗虚线表示服务器状态转换。

接收表示该状态转换在接受到什么分节时发生，发送表示该状态转换会发送什么分节

例子1 **客户端建立连接**：

当某个应用程序在CLOSED状态下执行主动打开时，TCP将发送一个SYN，且新的状态是SYN_SENT。如果这个TCP接着接收到一个带ACK的SYN，它将发送一个ACK，且新的状态是ESTABLISHED，这个状态通常是数据传送发生时的状态

例子2 **客户端、服务器断开连接**：

如果某个应用进程在接收到一个FIN之前调用close（主动关闭），则转换到FIN_WAIT_1状态

但如果某个应用进程在ESTABLISHED状态期间接收到一个FIN（被动关闭），那就转换到CLOSE_WAIT状态

![image-20210603170310342](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603170310342.png)

**TIME_WAIT状态**

执行主动关闭的那端会经历该状态，持续时间是2MSL（两倍的最长分节生命期，MSL是任何IP数据报能够在因特网中存活的最长时间）

TIME_WAIT状态存在原因：

1. 可靠地实现TCP全双工连接的终止。假设最终的ACK丢失，服务器将重新发送它的最终FIN，所以客户端必须维护状态信息，以允许它重新发送最终的那个ACK
2. 允许老的重复分节在网络中消逝。假设关闭了一个TCP连接，过一段时间又在相同的IP地址和端口号之间建立另一个连接（称为前一个连接的化身，因为IP、端口号一样）。TCP必须防止来自某个连接的老的重复分组在该连接已终止后再次出现。为做到这一点，TCP就不给处于TIME_WAIT状态的连接发起新的化身。既然TIME_WAIT的持续时间的2MSL，就足以让某个方向上的分组最多存活MSL即被丢弃。通过该规则，就能保证每成功建立一个TCP连接，来自该连接先前化身的老的重复分组都已经在网络中消逝了。

### 完整流程

![image-20210603171012559](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603171012559.png)



应用进程写数据到一个TCP套接字中时发生的步骤：

![image-20210603185857129](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603185857129.png)

每一个TCP套接字有一个发送缓冲区。当某个应用进程调用write时，内核从该应用进程的缓冲区中复制所有数据到所写套接字的发送缓冲区，如果该套接字的发送缓冲区容不下该应用进程的所有数据，该应用进程将置于休眠状态。内核将不从write系统调用返回，直到应用进程缓冲区中所有数据都复制到套接字发送缓冲区

因此，从一个TCP套接字的write调用成功返回仅仅表示我们可以重新使用原来的应用进程缓冲区，并不表明对端的TCP或应用进程已接收到数据。

这一端的TCP提取套接字发送缓冲区中的数据并把它发送给对端TCP。对端TCP必须确认收到的数据，伴随来自对端的ACK的不断到达，本端TCP至此才能从套接字发送缓冲区中丢弃已确认的数据。TCP必须为已发送的数据保留一个副本，直到它被对端确认为止

本端TCP以MSS大小的块把数据传递给IP，同时给每个数据块安上一个TCP首部已构成TCP分节，IP给每个TCP分节安上一个IP首部以构成IP数据报，并按照其目的IP地址查找路由表项以确定外出接口，然后把数据报传递给相应的数据链路。

## 简单案例

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603191611037.png" alt="image-20210603191611037" style="zoom: 67%;" />



![image-20210603203304963](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603203304963.png)

**正常终止步骤**

1. 当我们键入EOF字符时，fgets返回一个空指针，于是str_cli函数返回
2. 当str_cli函数返回到客户的main函数时，main通过调用exit终止
3. 进程终止处理的部分工作是关闭所有打开的描述符，因此客户打开的套接字由内核关闭，这导致客户TCP发送一个FIN给服务器，服务器TCP则以ACK响应。至此，服务器套接字处于CLOSE_WAIT状态，客户套接字处于FIN_WAIT_2状态
4. 当服务器TCP接收FIN时，服务器子进程阻塞于readline调用，于是readline返回0，这导致str_echo函数返回服务器子进程的main函数
5. 服务器子进程通过调用exit来终止
6. 服务器子进程中打开的所有描述符随之关闭，由子进程来关闭已连接套接字会引发TCP连接终止序列的最后两个分节：一个从服务器到客户的FIN和一个从客户到服务器的ACK。至此连接完全终止，客户端套接字进入TIME_WAIT状态

### 客户端

```c
#include	"unp.h"

int
main(int argc, char **argv)
{
	int					sockfd;
	struct sockaddr_in	servaddr;

	if (argc != 2)
		err_quit("usage: tcpcli <IPaddress>");

    // 创建一个套接字，返回描述符
	sockfd = Socket(AF_INET, SOCK_STREAM, 0);

    // 填入服务器的IP地址和端口号
	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family = AF_INET;
	servaddr.sin_port = htons(SERV_PORT);
	Inet_pton(AF_INET, argv[1], &servaddr.sin_addr);

    // 和服务器建立TCP连接
	Connect(sockfd, (SA *) &servaddr, sizeof(servaddr));

	str_cli(stdin, sockfd);

	exit(0); // 终止程序运行，Unix在进程终止时总是关闭该进程所有打开的描述符
}

void
str_cli(FILE *fp, int sockfd)
{
    char	sendline[MAXLINE], recvline[MAXLINE];

    while (Fgets(sendline, MAXLINE, fp) != NULL) {

        Writen(sockfd, sendline, strlen(sendline));

        if (Readline(sockfd, recvline, MAXLINE) == 0)
            err_quit("str_cli: server terminated prematurely");

        Fputs(recvline, stdout);
    }
}
```

客户调用st_cli函数，该函数将阻塞于fgets调用，因为我们还未输入任何数据。

### 服务器端

```c
#include	"unp.h"

int main(int argc, char **argv)
{
	int					listenfd, connfd;
	pid_t				childpid;
	socklen_t			clilen;
	struct sockaddr_in	cliaddr, servaddr;

    // 创建套接字
	listenfd = Socket(AF_INET, SOCK_STREAM, 0);

	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family      = AF_INET;
	servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
	servaddr.sin_port        = htons(SERV_PORT);

    // 调用bind函数绑定端口号
	Bind(listenfd, (SA *) &servaddr, sizeof(servaddr));

    // 把套接字转换成一个监听套接字，这样来自客户端的外来连接可以在该套接字由内核接收
	Listen(listenfd, LISTENQ);

	for ( ; ; ) {
		clilen = sizeof(cliaddr);
        // 服务器进程在accept调用中被置于休眠状态，等待某个客户连接的到达并被内核接受
        // TCP使用三次握手来建立连接，握手完毕后accept返回。一个已连接描述符connfd
        // 该描述符用于与新连接的那个客户通信
        // accept为每个连接到本服务器的客户返回一个新描述符
        // 该服务器一次只能处理一个客户。如果多个客户连接同时到达，系统内核在某个最大数量的限制下把他们排入队列，然后每次返回一个给accept函数
		connfd = Accept(listenfd, (SA *) &cliaddr, &clilen);

		if ( (childpid = Fork()) == 0) {	
			Close(listenfd);	
			str_echo(connfd);	
			exit(0);
		}
        // 服务器调用close关闭与客户的连接，引发四次挥手操作。（当然这里引用计数不为0，实际close操作在子进程完成）
		Close(connfd);			
	}
}

void str_echo(int sockfd)
{
    ssize_t		n;
    char		buf[MAXLINE];

    again:
    // 如果客户关闭连接，那么接收到客户的FIN将导致服务器子进程的read函数返回0，将导致str_echo返回，从而终止子进程
    while ( (n = read(sockfd, buf, MAXLINE)) > 0)
        Writen(sockfd, buf, n);

    if (n < 0 && errno == EINTR)
        goto again;
    else if (n < 0)
        err_sys("str_echo: read error");
}
```

当服务器端accept函数返回时，服务器调用fork，再有子进程调用str_echo。该函数调用readline，readline调用read，而read在等待客户送入一行文本期间阻塞。

另一方面，服务器父进程再次调用accept并阻塞，等待下一个客户连接。

### 并发服务器

主服务器循环通过派生一个子进程来处理每个新的连接

当服务器接收并接受这个客户的连接，它fork一个自身的副本，让子进程来处理该客户的请求

![image-20210603172737267](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603172737267.png)

TCP无法仅仅通过查看目的端口号来分离外来的分节到不同的端点，它必须查看套接字对的所有四个元素才能确定由哪个端点接收某个到达的分节

我们必须在服务器主机上区分监听套接字和已连接套接字。已连接套接字使用与监听套接字相同的本地端口（21）

对于同一个本地端口（21）存在三个套接字：

+ 如果一个分节来自客户端端口1500，目的地为服务器端端口21，被传递到第一个子进程
+ 如果一个分节来自客户端端口1501，目的地为服务器端端口21，被传递到第二个子进程
+ 所有目的端口为21的其他TCP分节被传递给拥有监听套接字的那个最初服务器（父进程）



是在同时有大量的客户连接到同一个服务器上时用于提供并发性的一种技术，每个客户连接都迫使服务器为他fork一个新的进程

```c
pid_t pid;
int listenfd, connfd;
listenfd = Socket();
Bind(listenfd);
Listen(listenfd, backlog);

for(;;) {
    connfd = Accept(listenfd);
    // 子进程进入if执行
    if ((pid == Fork()) == 0) {
        Close(listenfd);
        processHandler(connfd); // 处理请求
        Close(connfd);
        exit(0);
    }
    Close(connfd);
}
```

当一个连接建立时，accept返回，服务器接着调用fork，然后由子进程服务客户（提供已连接套接字connfd)，父进程则等待另一个连接（通过监听套接字listenfd)。既然新的客户由子进程提供服务，父进程就关闭已连接套接字。

对一个TCP套接字调用close会导致发送一个FIN，随后是正常的四次挥手，那么为什么父进程对connfd调用close没有终止它与客户的连接？因为每个文件或套接字都有一个引用计数（是当前打开着的引用该文件或套接字的描述符的个数）。套接字真正的清理和资源释放要等到引用计数值为0时才发生，即会在子进程也关闭connfd时发生。

![image-20210603200912778](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603200912778.png)

![image-20210603200853359](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603200853359.png)

![image-20210603200933250](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603200933250.png)

## 常见函数

### Socket函数

```c#
int socket(int family, in type, int protocol);
```

指定期望的通信协议类型（例如使用IPv4的TCP）

socket函数在成功时返回一个小的非负整数值，与文件描述符类似，称为套接字描述符，简称sockfd

### connect函数

TCP客户用connect函数来建立与TCP服务器的连接

```c
int connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen);
```

sockfd是由socket函数返回的套接字描述符

servaddr、addrlen是指向套接字地址结构的指针和该结构的大小（结构中含有服务器的IP地址和端口号）

客户调用connect函数时不需要调用bind函数，因为如果需要，内核会确定源ip地址，并选择一个临时端口作为源端口

如果是TCP套接字，调用connect函数将触发TCP的三次握手过程

### bind函数

```c
int bind(int sockfd, const struct sockaddr *myaddr, socklen_t addrlen);
```

myaddr、addrlen是指向套接字地址结构的指针和该结构的大小（可以指定要绑定的IP地址和端口号），如果要绑定IP地址，则该IP地址必须属于其所在主机的网络接口之一

### listen函数

```c
int listen(int sockfd, int backlog);
```

listen函数仅由TCP服务器调用，作用：

1. 当socket函数创建一个套接字时，它被假设为一个主动套接字，也就是一个调用connect发起连接的客户套接字。listen函数把一个未连接的套接字转换成一个被动套接字，指示内核应接受指向该套接字的连接请求。即调用listen函数导致套接字从CLOSED状态转换成LISTEN状态
2. 本函数第二个参数规定内核应该为相应套接字排队的最大连接个数

内核为任何一个给定的监听套接字维护两个队列：

1. **未完成连接队列**。每个这样的SYN分节对应其中的一项：已由某个客户发出并到达服务器，而服务器正在等待完成相应的TCP三次握手过程。这些套接字处于SYN_RCVD状态（在三次握手正常完成的前提下，未完成连接队列中的任何一项在其中的存留时间就是一个RTT)
2. **已完成连接队列**。每个已完成TCP三次握手过程的客户对应其中一项。这些套接字处于ESTABLISHED状态

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603193151337.png" alt="image-20210603193151337" style="zoom:67%;" />

每当在未完成连接队列中创建一项时，来自监听套接字的参数就复制到即将建立的连接中。连接的创建机制是完全自动的，无需服务器进程插手。

当来自客户的SYN到达时，TCP在未完成连接队列中创建一个新项，然后响应三次握手的第二个分节：服务器的SYN响应，其中捎带对客户SYN的ACK。这一项一直保留在未完成连接队列中，直到三次握手的第三个分节（客户对服务器SYN的ACK）到达或该项超时为止。如果三次握手正常完成，该项就从未完成连接队列移动到已完成连接队列的队尾。当进程调用accept时，已完成连接队列中的队头项就返回给进程，或者如果该队列为空，那么进程将处于休眠状态，直到TCP在该队列中放入一项才唤醒他

在三次握手完成之后，但在服务器调用accept之前到达的数据应由服务器TCP排队，最大数据量为相应已连接套接字的接受缓冲区大小

### accept函数

```c
int accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen);
```

accept函数由TCP服务器调用，用于从已完成连接队列队头返回下一个已完成连接。如果已完成连接队列为空，那么进程置为休眠状态。

如果accept成功。那么其返回值是由内核自动生成的一个全新描述符，代表与所返回客户的TCP连接

称第一个参数为监听套接字描述符，返回值为已连接套接字描述符。一个服务器通常仅仅创建一个监听套接字，他在该服务器的生命周期内一直存在，内核为每个由服务器进程接受的客户连接创建一个已连接套接字（对于他的三次握手过程已经完成）。当服务器完成对某个给定客户的服务时，相应的已连接套接字就被关闭。

### close函数

```c
int close(int sockfd);
```

关闭套接字，终止TCP连接

默认行为是把该套接字标记成已关闭，然后立即返回到调用进程。该套接字描述符不能再由调用进程使用，也就是说它不能再作为read或write的参数。

然而TCP将尝试发送已排队等待发送到对端的任何数据，发送完毕后发生的是正常的TCP连接四次挥手

### fork函数

fork函数是Unix派生新进程的唯一方法

```c
pid_t fork(void); // 返回在子进程中为0，在父进程中为子进程ID，若出错为-1
```

调用fork函数一次，它返回两次：

1. 它在调用进程（父进程）中返回一次，返回值是新派生进程（子进程）的进程ID
2. 在子进程又返回一次，返回值是0

因此返回值本身告知当前进程是父进程还是子进程

父进程中调用fork之前打开的所有描述符在fork返回之后由子进程分享。如果父进程在调用accept之后调用fork，所接受的已连接套接字随后就在父进程和子进程之间共享。通常情况下，子进程接着读写这个已连接套接字，父进程则关闭这个已连接套接字

fork函数典型用法：

1. 一个进程创建一个自身的副本，这样每个副本都可以在来一个副本执行其他任务的同时处理各自的某个操作，这是网络服务器的典型用法。
2. 一个进程想要执行另一个程序。该进程调用fork创建一个自身的副本，其中一个副本（子进程）调用exec把自身替换成新的程序。是shell等程序的典型用法。

# IO模型

## 概述

首先看看服务端处理网络请求的典型过程：



![服务端处理网络请求流程图](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a72396ad36eda)



可以看到，主要处理步骤包括：

- 1、获取请求数据 客户端与服务器建立连接发出请求，服务器接受请求（1-3）
- 2、构建响应 当服务器接收完请求，并在用户空间处理客户端的请求，直到构建响应完成（4）
- 3、返回数据 服务器将已构建好的响应再通过内核空间的网络I/O发还给客户端（5-7）



输入操作通常包括两个不同的阶段：

1. 等待数据准备好
2. 从内核向进程复制数据

对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。

**UDP套接字编程**

下面的IO模型拿UDP举例，因为数据准备好读取的概念比较简单：要么整个数据报已经收到，要么还没有。所以先简述UDP套接字编程

客户不与服务器建立连接，而是只使用sendto函数给服务器发送数据报，其中必须指定目的地。类似的，服务器不接受来自客户的连接，而是只管调用recvfrom函数，等待来自某个客户的数据到达。recvfrom将与所接收的数据报一道返回客户的协议地址，因此服务器可以把响应发送给正确的客户。

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604183925749.png" alt="image-20210604183925749" style="zoom:67%;" />

**阻塞和非阻塞**

描述的是用户线程调用内核IO操作的方式：阻塞是指IO操作需要彻底完成后才返回到用户空间；而非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。阻塞调用会一直等待远程数据就绪再返回，直到读取结束。而非阻塞无论在什么情况下都会立即返回，虽然非阻塞大部分时间不会被block，但是它仍要求进程不断地去主动询问kernel是否准备好数据，也需要进程主动地再次调用recvfrom来将数据拷贝到用户内存。

- 阻塞调用与非阻塞调用
    - 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回
    - 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程

两者的最大区别在于被调用方在收到请求到返回结果之前的这段时间内，调用方是否一直在等待。阻塞是指调用方一直在等待而且别的事情什么都不做。非阻塞是指调用方先去忙别的事情

**同步和异步**

描述的是用户线程与内核的交互方式：同步是指用户线程发起IO请求后需要等待或者轮询内核IO操作完成后才能继续执行；而异步是指用户线程发起IO请求后仍继续执行，当内核IO操作完成后会通知用户线程，或者调用用户线程注册的回调函数。

- 同步处理与异步处理

    - 同步处理是指被调用方得到最终结果之后才返回给调用方
    - 异步处理是指被调用方先返回应答，然后再计算调用结果，计算完最终结果后再通知并返回给调用方

    **阻塞、非阻塞的讨论对象是调用者**

    **同步、异步的讨论对象是被调用者**

### IO模型比较

+ 同步IO操作：导致请求进程阻塞，直到IO操作完成
+ 异步IO操作：不导致请求进程阻塞

前四种同步IO模型的主要区别在第一阶段，因为他们第二阶段是一样的：在数据从内核复制到调用者缓冲区期间，进程阻塞与recvfrom调用。相反，异步IO模型在这两阶段都要处理

![img](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a723ae2b8fb2a)

阻塞式IO模型、非阻塞式IO模型、IO复用模型、信号驱动IO模型都是同步IO模型，因为其中真正的IO操作(recvfrom)将阻塞进程。

![image-20210207165051430](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207165051430.png)

## 阻塞式IO模型

把recvfrom视为系统调用，因为要区分应用进程和内核。它一般都会从在应用进程空间中运行切换到在内核空间中运行，一段时间后再切换回来

进程调用recvfrom，其系统调用直到数据报到达且被复制到应用进程的缓冲区中或者发生错误才返回。进程在从调用recvfrom开始到它返回的整段时间内是被阻塞的。recvfrom成功返回后，应用进程开始处理数据报。

![阻塞式I/O模型](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a72396a811072)



![image-20210207152811758](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207152811758.png)



- 同步阻塞IO模型是最简单的IO模型，用户线程在内核进行IO操作时被阻塞。

- 用户线程通过系统调用read发起IO读操作，由用户空间转到内核空间。内核等到数据包到达后，然后将接收的数据拷贝到用户空间，完成read操作。

- 服务端采用单线程，当 accept 一个请求后，在read调用阻塞时，将无法 accept 其他请求（必须等上一个请求处理完 ）（无法处理并发）

- 套接字的读写方法，默认是阻塞的。例如read方法要传递进去一个参数n，代表最多读取n个字节后再返回，如果一个字节都没有，则线程卡顿直到新的数据到来或连接关闭，read方法才能返回。

- write方法一般不会阻塞，除非内核为套接字分配的写缓冲区满了，write方法才会阻塞，直到缓冲区中有空间空闲出来

- 即用户需要等待read将socket中的数据读取到buffer后，才继续处理接收的数据。整个IO请求的过程中，用户线程是被阻塞的，这导致用户在发起IO请求时，不能做任何事情，对CPU的资源利用率不够。

- 阻塞IO意味着当我们发起一次IO操作后一直等待成功或失败之后才返回，在这期间程序不能做其它的事情。阻塞IO操作只能对单个文件描述符进行操作

```java
{
    read(socket， buffer);
    process(buffer);
}
```



## 非阻塞式IO模型

进程把一个套接字设置成非阻塞是通知内核：当所请求的IO操作需要把本进程阻塞时，不阻塞了，而是返回一个错误

前三次调用recvfrom时没有数据可返回，因此内核立即返回一个RWOULDBLOCK错误。第四次调用recvfrom时已有一个数据报准备好，它被复制到应用进程缓冲区，于是recvfrom成功返回。我们接着处理数据。

当一个应用进程像这样对一个非阻塞描述符循环调用recvfrom时，称为轮询。应用程序持续轮询内核，以查看某个操作是否就绪，往往会消耗大量的CPU时间

![非阻塞式I/O模型](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a72396af89a4a)



![image-20210207153315652](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207153315652.png)

- 由于socket是非阻塞的方式，因此用户线程发起IO请求时立即返回。但并未读取到任何数据，用户线程需要不断地发起IO请求，直到数据到达后，才真正读取到数据，继续执行。
- 非阻塞IO通常发生在一个for循环当中，因为每次进行IO操作时要么IO操作成功，要么当IO操作会阻塞时返回错误EWOULDBLOCK/EAGAIN，然后再根据需要进行下一次的for循环操作，这种类似轮询的方式会浪费很多不必要的CPU资源，是一种糟糕的设计。
- 即用户需要不断地调用read，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，消耗了大量的CPU的资源。

```
{
    while(read(socket， buffer) != SUCCESS);
    process(buffer);
}
```

## IO复用模型

**比喻** 放了一堆鱼竿，在岸边一直守着这堆鱼竿，没有鱼上钩就玩手机

例子：

Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而  **I/O 多路复用** 就是为了解决这个问题而出现的。

redis的io模型主要是基于epoll实现的，不过它也提供了 select和kqueue的实现，默认采用epoll。

+ 优先选择时间复杂度为O(1)的IO复用函数作为底层实现，例如epoll
+ 以select系统调用为保底，时间复杂度O(N)
+ 基于react设计模式监听IO事件

多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。

**这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。**采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。



**原理**

- IO多路复用模型是建立在内核提供的多路分离函数select基础之上的，使用select函数可以避免同步非阻塞IO模型中轮询等待的问题。
- IO 多路复用实现一个线程可以监视多个文件句柄；
- 一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；
- 没有文件句柄就绪就会阻塞应用程序，交出CPU。
- 多路是指网络连接，复用指的是同一个线程
- 服务器端采用单线程通过 select/poll/epoll 等系统调用获取 fd 列表，遍历有事件的 fd 进行 accept/recv/send ，使其能支持更多的并发连接请求。





多路网络连接复用一个io线程。

单线程或单进程同时监测若干个文件描述符是否可以执行IO操作的能力。

一种可以在单线程/进程中处理多个事件流的方法

I/O多路复用（multiplexing）的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作

进程需要一种预先告知内核的能力，使得内核一旦发现进程指定的一个或多个IO条件就绪（输入已经准备好或描述符已能承接更多的输出），它就通知进程。

适用于：

+ 客户处理多个描述符（通常是交互式输入和网络套接字）
+ TCP服务器既要处理监听套接字，又要处理已连接套接字

select, poll, epoll 都是I/O多路复用的具体的实现

阻塞在这两个系统调用中的某一个之上，而不是阻塞在真正的IO系统调用上。

我们阻塞于select调用，等待数据报套接字变为可读。当select返回套接字可读这一条件时，我们调用recvfrom把所读数据复制到应用进程缓冲区。

使用select的优势在于我们可以等待多个描述符就绪。

服务器端采用单线程通过 select/poll/epoll 等系统调用获取 fd 列表，遍历有事件的 fd 进行 accept/recv/send ，使其能支持更多的并发连接请求。

![I/O复用模型](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a72396cc78d98)

![image-20210207153611203](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207153611203.png)

用户首先将需要进行IO操作的socket添加到select中，然后阻塞等待select系统调用返回。当数据到达时，socket被激活，select函数返回。用户线程正式发起read请求，读取数据并继续执行。

从流程上来看，使用select函数进行IO请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。但是，使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在**同一个线程内同时处理多个IO请求的目的**。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

```java
//将socket添加到select监视中
select(socket);

while(1) {
    sockets = select();
    //一直调用select获取被激活的socket，一旦socket可读，便调用read函数将socket中的数据读取出来。
    for(socket in sockets) {
        if(can_read(socket)) {
            read(socket， buffer);
            process(buffer);
        }
    }
}
```

虽然上述方式允许单线程内处理多个IO请求，但是每个IO请求的过程还是阻塞的（在select函数上阻塞），平均时间甚至比同步阻塞IO模型还要长。如果用户线程只注册自己感兴趣的socket或者IO请求，然后去做自己的事情，等到数据到来时再进行处理，则可以提高CPU的利用率。

IO多路复用模型使用了Reactor设计模式实现了这一机制。

![image-20210207154330581](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207154330581.png)

**Reactor设计模式**

EventHandler抽象类表示IO事件处理器，它拥有IO文件句柄Handle（通过get_handle获取），以及对Handle的操作handle_event（读/写等）。继承于EventHandler的子类可以对事件处理器的行为进行定制。Reactor类用于管理EventHandler（注册、删除等），并使用handle_events实现事件循环，不断调用同步事件多路分离器（一般是内核）的多路分离函数select，只要某个文件句柄被激活（可读/写等），select就返回（阻塞），handle_events就会调用与文件句柄关联的事件处理器的handle_event进行相关操作。

![image-20210207154426634](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207154426634.png)

通过Reactor的方式，可以将用户线程轮询IO操作状态的工作统一交给handle_events事件循环进行处理。用户线程注册事件处理器之后可以继续执行做其他的工作（异步），而Reactor线程负责调用内核的select函数检查socket状态。当有socket被激活时，则通知相应的用户线程（或执行用户线程的回调函数），执行handle_event进行数据读取、处理的工作。由于select函数是阻塞的，因此多路IO复用模型也被称为异步阻塞IO模型。注意，这里的所说的阻塞是指select函数执行时线程被阻塞，而不是指socket。一般在使用IO多路复用模型时，socket都是设置为NONBLOCK的，不过这并不会产生影响，因为用户发起IO请求时，数据已经到达了，用户线程一定不会被阻塞。

用户线程使用IO多路复用模型的伪代码描述为：

```c
void UserEventHandler::handle_event() {
    if(can_read(socket)) {
        read(socket， buffer);
        process(buffer);
    }
}

//用户需要重写EventHandler的handle_event函数进行读取数据、处理数据的工作，用户线程只需要将自己的EventHandler注册到Reactor即可。
{
	Reactor.register(new UserEventHandler(socket));
}
```

Reactor中handle_events事件循环的伪代码大致如下。

事件循环不断地调用select获取被激活的socket，然后根据获取socket对应的EventHandler，执行器handle_event函数即可。

```c
Reactor::handle_events() {
    while(1) {
        sockets = select();
        for(socket in sockets) {
        	get_event_handler(socket).handle_event();
        }
    }
}
```

## 信号驱动IO模型

**比喻** 鱼竿上系了个铃铛，当铃铛响，就知道鱼上钩，然后可以专心玩手机

信号驱动IO是利用信号机制，让内核告知应用程序文件描述符的相关事件。

但信号驱动IO在网络编程的时候通常很少用到，因为在网络环境中，和socket相关的读写事件太多了，比如下面的事件都会导致SIGIO信号的产生：

1. TCP连接建立
2. 一方断开TCP连接请求
3. 断开TCP连接请求完成
4. TCP连接半关闭
5. 数据到达TCP socket
6. 数据已经发送出去(如：写buffer有空余空间)

上面所有的这些都会产生SIGIO信号，但我们没办法在SIGIO对应的信号处理函数中区分上述不同的事件，SIGIO只应该在IO事件单一情况下使用，比如说用来监听端口的socket，因为只有客户端发起新连接的时候才会产生SIGIO信号。

也可以用信号，让内核在描述符就绪时发送SIGIO信号通知我们

首先开启套接字的信号驱动式IO功能，并通过sigaction系统调用安装一个信号处理函数。该系统调用立即返回，我们进程继续工作，即没有被阻塞。当数据报准备好读取时，内核就为该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它读取数据报。

无论如何处理SIGIO信号，这种模型优势在于等待数据报到达期间进程不被阻塞。主循环可以继续执行，只要等待来自信号处理函数的通知：既可以是数据已准备好被处理，也可以是数据报已准备好被读取

![信号驱动式I/O模型](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a72396ceccf52)

**缺点**

- 信号I/O在大量IO操作时可能会因为信号队列溢出导致没法通知
- 信号驱动I/O尽管对于处理UDP套接字来说有用，即这种信号通知意味着到达一个数据报，或者返回一个异步错误。但是，对于TCP而言，信号驱动的I/O方式近乎无用，因为导致这种通知的条件为数众多，每一个来进行判别会消耗很大资源，与前几种方式相比优势尽失

## 异步IO模型

告知内核启动某个操作，并让内核在整个操作（包括将数据从内核复制到我们自己的缓冲区）完成后通知我们。

与信号驱动模型的主要区别在于：信号驱动IO由内核通知我们何时可以启动一个IO操作，而异步IO模型是由内核通知我们IO操作何时完成。

我们调用aio_read函数，给内核传递描述符、缓冲区指针、缓冲区大小和文件偏移，并告诉内核当整个操作完成时如何通知我们。

本例子中我们假设要求内核在操作完成时产生某个信号。该信号直到数据已复制到应用进程缓冲区才产生。

![异步I/O模型](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/165a723ae253818f)

“真正”的异步IO需要操作系统更强的支持。在IO多路复用模型中，事件循环将文件句柄的状态事件通知给用户线程，由用户线程自行读取数据、处理数据。而在异步IO模型中，当用户线程收到通知时，数据已经被内核读取完毕，并放在了用户线程指定的缓冲区内，内核在IO完成后通知用户线程直接使用即可。



![image-20210207154623278](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210207154623278.png)

异步IO模型中，用户线程直接使用内核提供的异步IO API发起read请求，且发起后立即返回，继续执行用户线程代码。

目前操作系统对异步IO的支持并非特别完善，更多的是采用IO多路复用模型模拟异步IO的方式（IO事件触发时不直接通知用户线程，而是将数据读写完毕后放到用户指定的缓冲区中）。

异步IO和信号驱动IO差不多，但它比信号驱动IO可以多做一步：相比信号驱动IO需要在程序中完成数据从用户态到内核态(或反方向)的拷贝，异	步IO可以把拷贝这一步也帮我们完成之后才通知应用程序。

异步IO是完全完成了数据的拷贝之后才通知程序进行处理，没有阻塞的数据读写过程。

这里面的读取操作的语义与上面的几种模型都不同。这里的读取操作(aio_read)会通知内核进行读取操作并将数据拷贝至进程中，完事后通知进程整个操作全部完成（绑定一个回调函数处理数据）。读取操作会立刻返回，程序可以进行其它的操作，所有的读取、拷贝工作都由内核去做，做完以后通知进程，进程调用绑定的回调函数来处理数据。

# select、poll、epoll函数

## 概述

### 文件描述符

文件描述符是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。

### 进程阻塞

 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得了CPU资源），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。

### 缓存I/O

 缓存I/O又称为标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存中，即数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

### 总结

select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。

## select函数

### 概述

该函数允许进程指示内核等待多个事件中的任何一个发生，并只在有一个或多个事件发生或经历一段指定的时间后才唤醒它。

例子：我们可以调用select，告知内核仅在下列情况发生时才返回：

+ 集合{1，4，5}中的任何描述符准备好读；
+ 集合{2，7}中的任何描述符准备好写；
+ 集合{1，4}中的任何描述符有异常条件待处理；
+ 已经历10.2s

也就是说，我们调用select告知内核对哪些描述符（就读、写或异常条件）感兴趣，以及等待多长时间。

```c
int select(int maxfdp1, fd_set *readset, fd_set *writeset, fd_set *exceptset, const struct timeval *timeout);
```

timeout告知内核等待所指定描述符中的任何一个就绪可花多长时间，可赋值为三种情况：

1. 永远等待下去：仅在有一个描述符准备好IO才返回。为此，我们把该参数设置为空指针。
2. 等待一段固定时间：在有一个描述符准备好IO才返回，但是不超过指定参数的时间。
3. 根本不等待：检查描述符后立即返回，这称为轮询。参数指定为0.

fd_set可以理解为一个集合，这个集合中存放的是文件描述符，即文件句柄。readset、writeset、exceptset指定我们要让内核测试读、写和异常条件的描述符，如果对某一个条件不感兴趣，就可以把它设为空指针。

maxfdp1参数指定待测试的描述符个数，它的值是待测试的最大描述符加1。

当函数返回时，结果将指示哪些描述符已就绪。

头文件<sys/select.h>中定义的FD_SETSIZE常量值是数据类型fd_set中的描述符总数（通常是1024）

当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读。

使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

### 理解

+ 这里的select相当于一个“代理”，调用select以后进程会被select阻塞，这时候在内核空间内select会监听指定的多个datagram (如socket连接)，如果其中任意一个数据就绪了就返回。此时程序再进行数据读取操作，将数据拷贝至当前进程内。由于select可以监听多个socket，我们可以用它来处理多个连接。
+ 在select模型中每个socket一般都设置成non-blocking，虽然等待数据阶段仍然是阻塞状态，但是它是被select调用阻塞的，而不是直接被I/O阻塞的。select底层通过轮询机制来判断每个socket读写是否就绪。
+ 当然select也有一些缺点，比如底层轮询机制会增加开销、支持的文件描述符数量过少等。为此，Linux引入了epoll作为select的改进版本。
+ select、poll、epoll本质上也都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。

### 优缺点

它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。

- 使用事件轮询API的select函数，输入是read_fds & write_fds；输出是与之对应的可读可写事件，同时还提供了timeout参数。
- 如果期间没有任何事件到来，那么最多等待timeout的值的时间，线程处于阻塞状态。
- 一旦其间有任何事件到来，就立即返回。时间过了之后还是没有任何事件，就立即返回。
- 拿到事件后，线程可以继续挨个处理相应事件，处理完了继续轮询，于是线程就进入了一个死循环，我们称循环为时间循环，一个循环为一个周期。

**select缺点**
select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：

- 单个进程所打开的FD是有限制的，通过 FD_SETSIZE 设置，默认1024 ;为了减少数据拷贝带来的性能损坏，内核对被监控的fd_set集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)
- 每次调用 select，都需要把 fd_set集合从用户态拷贝到内核态，这个开销在 fd_set很多时会很大；需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大
- 对 socket 扫描时是线性扫描，采用轮询的方法，效率较低（高并发)。当套接字比较多的时候，每次select()都要通过遍历FD_SET中SIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。

### 客户端程序

![image-20210604193654363](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604193654363.png)

8-13行：**调用select**

我们只需要一个用于检查可读性的描述符集。该集合由FD_ZERO初始化，并用FD_SET打开两位：一位对应于标准IO文件指针fp，一位对应于套接字sockfd。fileno函数把标准IO文件指针转换为对应的描述符。因为select函数只工作在描述符上。

计算两个描述符中的较大者后，调用select。在该调用中，写集合指针和异常集合指针都是空指针。最后一个参数（时间限制）也是空指针，因为我们希望本调用阻塞到某个描述符就绪为止。

14-18行：**处理可读套接字**

如果在select返回时套接字是可读的，就执行对应操作

19-23行：**处理可读输入**

如果标准输入可读，就执行相应操作

### 服务器程序

把之前的案例重写为使用select来处理任意个客户的单进程程序。

![image-20210604194813136](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604194813136.png)

服务器只维护一个读描述符集，假设服务器是前台启动的，那么描述符0、1、2分别被置为标准输入、标准输出、标准错误输出。所以监听套接字的第一个可用描述符是3

client数组中含有每个客户的已连接套接字描述符，初始化为-1

![image-20210604195011425](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604195011425.png)

当第一个客户与服务器建立连接时，监听描述符变为可读，我们的服务器于是调用accept

![image-20210604195113032](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604195113032.png)

我们的服务器必须在client数组中记住每个新的已连接描述符，并把他加到描述符集中去：

![image-20210604195226480](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604195226480.png)

随后第二个客户与服务器建立连接：

![image-20210604195258834](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604195258834.png)

接着假设第一个客户终止连接。该客户的TCP发生一个FIN，使得描述符4变得可读。当服务器读这个已连接套接字时，read将返回0。于是关闭该套接字并且把client[0]置为-1，把描述符集中描述符4的位设置位0

![image-20210604195506423](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210604195506423.png)

总之，有客户到达时，我们在client数组中的第一个可用项（值为-1的项）中记录其已连接套接字的描述符。并且还必须把这个已连接描述符加到读描述符集中

```c
listenfd = Socket(AF_INET, SOCK_STREAM, 0);
Bind(listenfd);
Listen(listenfd, LISTENQ);

for ( ; ; ) {
    nready = Select(maxfd+1, &rset, NULL, NULL, NULL);
    // 处理新连接接入
    if (FD_ISSET(listenfd, &rset)) {	
        connfd = Accept(listenfd);
    }
    // 检查现有连接
    for (i = 0; i <= maxi; i++) {	
        if (FD_ISSET(sockfd, &rset)) {
            if ( (n = Read(sockfd, buf, MAXLINE)) == 0) {
            } else
                Writen(sockfd, buf, n);
        }
    }
}
```

- **创建监听套接字并调用select进行初始化**
- **阻塞于select**：select等待某个事件发生：或是新连接建立，或是数据、FIN或RST到达
- **accept新的连接**：如果监听套接字变为可读，则已建立一个新的连接。我们调用accept并更新相应的数据结构。
- **检查现有连接**：对于每个现有的客户连接，我们要测试其描述符是否在select返回的描述集里。

## poll函数

```c
int poll(struct pollfd *fdarray, unsigned long nfds, int timeout);
```

- fdarray是指向一个结构数组的指针，每个数组元素都是一个pollfd结构，用于指定测试某个给定描述符fd的条件。还是三种：处理输入的、处理输出的、处理错误的
- nfds指定了结构数组中元素的个数
- timeout指定poll函数返回前等待多长时间
- 当发生错误时，poll函数返回值是-1、若定时器到时之前没有任何描述符就绪，则返回0、否则返回就绪描述符的个数（即revents成员值非0的描述符个数）



poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， 但是它没有最大连接数的限制，原因是它是基于链表来存储的。

**poll缺点**
它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有缺点：

- 每次调用 poll ，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大；
- 对 socket 扫描是线性扫描，采用轮询的方法，效率较低（高并发时）

### 服务器

```c
int main(int argc, char **argv)
{
	listenfd = Socket(AF_INET, SOCK_STREAM, 0);
	Bind();
	Listen(listenfd, LISTENQ);

    struct pollfd		client[OPEN_MAX];
    
	client[0].fd = listenfd;
	client[0].events = POLLRDNORM;
	for (i = 1; i < OPEN_MAX; i++)
		client[i].fd = -1;						

	for ( ; ; ) {
		nready = Poll(client, maxi+1, INFTIM);

        // 新连接
		if (client[0].revents & POLLRDNORM) {	
			for (i = 1; i < OPEN_MAX; i++)
				if (client[i].fd < 0) {
					client[i].fd = connfd;	
					break;
				}
			client[i].events = POLLRDNORM;
		}
		// 检查某个现有连接上的数据
		for (i = 1; i <= maxi; i++) {	
			if ( (sockfd = client[i].fd) < 0)
				continue;
			if (client[i].revents & (POLLRDNORM | POLLERR)) {
				if ( (n = read(sockfd, buf, MAXLINE)) < 0) {
						Close(sockfd);
						client[i].fd = -1;
				} else if (n == 0) {
                    // 代表连接关闭
                    Close(sockfd);
					client[i].fd = -1;
                } else {
                    Writen(sockfd, buf, n);
                }
			}
		}
	}
}
```

+ 分配pollfd结构数组
+ 初始化。把client数组的第一项用于监听套接字，并把其余各项的描述符成员置为-1
+ 调用poll，检查新的连接。我们调用poll以等待新的连接或者现有连接上有数据可读。当一个新的连接被接受后，我们在client数组中查找第一个描述符成员为负的可用项。注意，从下标1开始搜索，因为client[0]固定用于监听套接字。
+ 检查某个现有连接上的数据。

## epoll函数

### 简介

epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是**事件驱动（每个事件关联上fd）**的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)）

**epoll的优点**

- 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）；
- 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll；
- 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。

### 了解

epoll 全称 eventpoll，是 linux 内核实现IO多路复用（IO multiplexing）的一个实现。IO多路复用的意思是在一个操作里同时监听多个输入输出源，在其中一个或多个输入输出源可用的时候返回，然后对其的进行读写操作。

epoll 监听的 fd（file descriptor）集合是常驻内核的，它有 3 个系统调用 （*epoll_create*, *epoll_wait*, *epoll_ctl*），通过 *epoll_wait* 可以多次监听同一个 fd 集合，只返回可读写那部分

select 只有一个系统调用，每次要监听都要将其从用户态传到内核，有事件时返回整个集合。

从性能上看，如果 fd 集合很大，用户态和内核态之间数据复制的花销是很大的，所以 select 一般限制 fd 集合最大1024。

从使用上看，epoll 返回的是可用的 fd 子集，select 返回的是全部，哪些可用需要用户遍历判断。

### 原理

![](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/20140911103834_133.jpg)

设想一个场景：有100万用户同时与一个进程保持着TCP连接，而每一时刻只有几十个或几百个TCP连接是活跃的(接收TCP包)，也就是说在每一时刻进程只需要处理这100万连接中的一小部分连接。那么，如何才能高效的处理这种场景呢？进程是否在每次询问操作系统收集有事件发生的TCP连接时，把这100万个连接告诉操作系统，然后由操作系统找出其中有事件发生的几百个连接呢？实际上，在Linux2.4版本以前，那时的select或者poll事件驱动方式是这样做的。

  这里有个非常明显的问题，即在某一时刻，进程收集有事件的连接时，其实这100万连接中的大部分都是没有事件发生的。因此如果每次收集事件时，都把100万连接的套接字传给操作系统(这首先是用户态内存到内核态内存的大量复制)，而由操作系统内核寻找这些连接上有没有未处理的事件，将会是巨大的资源浪费，然后select和poll就是这样做的，因此它们最多只能处理几千个并发连接。而epoll不这样做，它在Linux内核中申请了一个简易的文件系统，把原先的一个select或poll调用分成了3部分：

```c++
int epoll_create(int size);  
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);  
```

1. 调用epoll_create建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；

2. 调用epoll_ctl向epoll对象中添加这100万个连接的套接字；

3. 调用epoll_wait收集发生事件的连接。

  这样只需要在进程启动时建立1个epoll对象，并在需要的时候向它添加或删除连接就可以了，因此，在实际收集事件时，epoll_wait的效率就会非常高，因为调用epoll_wait时并没有向它传递这100万个连接，内核也不需要去遍历全部的连接。

### epoll_create

当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关，如下所示：

```c
struct eventpoll {
　　...
　　/*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，
　　也就是这个epoll监控的事件*/
　　struct rb_root rbr;
　　/*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/
　　struct list_head rdllist;
　　...
};
```

调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个rdllist双向链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个rdllist双向链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。

  所有添加到epoll中的事件都会与设备(如网卡)驱动程序建立回调关系，也就是说相应事件的发生时会调用这里的回调方法。这个回调方法在内核中叫做ep_poll_callback，它会把这样的事件放到上面的rdllist双向链表中。
在epoll中对于每一个事件都会建立一个epitem结构体，如下所示：

```c
struct epitem {
　　...
　　//红黑树节点
　　struct rb_node rbn;
　　//双向链表节点
　　struct list_head rdllink;
　　//事件句柄等信息
　　struct epoll_filefd ffd;
　　//指向其所属的eventepoll对象
　　struct eventpoll *ep;
　　//期待的事件类型
　　struct epoll_event event;
　　...
}; // 这里包含每一个事件对应着的信息。
```

当调用epoll_wait检查是否有发生事件的连接时，只是检查eventpoll对象中的rdllist双向链表是否有epitem元素而已，如果rdllist链表不为空，则这里的事件复制到用户态内存（使用共享内存提高效率）中，同时将事件数量返回给用户。因此epoll_waitx效率非常高。epoll_ctl在向epoll对象中添加、修改、删除事件时，从rbr红黑树中查找事件也非常快，也就是说epoll是非常高效的，它可以轻易地处理百万级别的并发连接。

### 原理总结

- 一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。

- 执行epoll_create()时，创建了红黑树和就绪链表；

- 执行epoll_ctl()时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据；

- 执行epoll_wait()时立刻返回准备就绪链表里的数据即可。







# JAVA IO

## BIO

### 概述

BIO是一个同步并阻塞的IO模式，**传统的  java.io 包**，它基于流模型实现，提供了我们最熟知的一些 IO 功能，比如**File抽象、输入输出流**等。**交互方式是同步、阻塞的方式**，也就是说，在读取输入流或者写入输出流时，在读、写动作完成之前，线程会一直阻塞在那里，它们之间的调用是可靠的线性顺序。

![img](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/1890419bbba44f29b0f71f5598c5e111~tplv-k3u1fbpfcp-zoom-1.image)

blocking I/O。同步阻塞模型

由一个独立的Acceptor线程负责监听客户端的连接，它接收到客户端连接请求之后为每个客户端创建一个新的线程进行处理。

每个线程都需要创建独立的线程，当并发量大时，需要创建大量线程来处理连接，系统资源占用大

代码中的read操作是阻塞操作，如果连接之后，服务端一直不发送数据，将会一直阻塞当前线程，浪费资源。

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603081447596.png" alt="image-20210603081447596" style="zoom: 67%;" />

### 详解

当服务器进程运行时, 可能会同时监听到多个客户的连接请求。

每当一个客户进程执行以下代码：

```java
Socket client = new Socket("127.0.0.1", 8848);
```

就意味着在远程主机的 8848 端口上, 监听到了一个客户的连接请求。管理客户连接请求的任务是由操作系统来完成的。操作系统把这些连接请求存储在一个先进先出的队列中。当队列中的连接请求达到了队列的最大容量时, 服务器进程所在的主机会拒绝新的连接请求。只有当服务器进程通过 ServerSocket 的 accept() 方法从队列中取出连接请求, 使队列腾出空位时，队列才能继续加入新的连接请求。

 对于客户进程, 如果它发出的连接请求被加入到服务器的请求连接队列中, 就意味着客户与服务器的连接建立成功, 客户进程从 Socket 构造方法中正常返回。

当客户进程的 Socket构造方法返回成功, 表示客户进程的连接请求被加入到服务器进程的请求连接队列中。 虽然客户端成功返回 Socket对象， 但是还没跟服务器进程形成一条通信线路。必须在服务器进程通过 ServerSocket 的 accept() 方法从请求连接队列中取出连接请求，并返回一个Socket 对象后，服务器进程这个Socket 对象才与客户端的 Socket 对象形成一条通信线路。

ServerSocket 的 accept() 方法从连接请求队列中取出一个客户的连接请求，然后创建与客户连接的 Socket 对象, 并将它返回。如果队列中没有连接请求，accept() 方法就会一直等待，直到接收到了连接请求才返回。

 接下来，服务器从 Socket 对象中获得输入流和输出流，就能与客户交换数据。

服务器的主线程负责接收客户的连接, 每次接收到一个客户连接, 就会创建一个工作线程, 由它负责与客户的通信

### 示例程序

**Server**

```java
public class BIOServer {
    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(8848);
        while (true) {
            // 线程在accept调用时处于休眠状态，等待某个客户连接到达并被内核接受
            Socket socket = serverSocket.accept();
            new Thread(() -> handler(socket)).start();
        }
    }

    public static void handler(Socket socket) {
        InputStream inputStream = null;
        try {
            byte[] bytes = new byte[1024];
            inputStream = socket.getInputStream();
            int read;
            while ((read = inputStream.read(bytes)) != -1) {
                System.out.println(new String(bytes, 0, read));
            }
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            try {
                if (inputStream != null) {
                    inputStream.close();
                }
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
}
```

**Client**

```java
public class BIOClient {
    public static void main(String[] args) throws IOException {
        Socket client = new Socket("127.0.0.1", 8848);
        OutputStream outputStream = client.getOutputStream();
        String msg = "Hello Server!";
        outputStream.write(msg.getBytes(StandardCharsets.UTF_8));
        client.close();
    }
}
```

## NIO

### IO操作

应用程序的IO操作实际上不是物理设备级别的读写，而是缓存的复制。read和write两大系统调用都不负责数据在内核缓冲区和物理设备（如磁盘、网卡等）之间的交换。这个底层的读写交换操作是由操作系统内核（Kernel）来完成的。

外部设备的直接读写涉及操作系统的中断。发生系统中断时，需要保存之前的进程数据和状态等信息，结束中断之后，还需要恢复之前的进程数据和状态等信息。为了减少底层系统的频繁中断所导致的时间损耗、性能损耗，出现了内核缓冲区。操作系统会对内核缓冲区进行监控，等待缓冲区达到一定数量的时候，再进行IO设备的中断处理，集中执行物理设备的实际IO操作，通过这种机制来提升系统的性能。

上层应用使用read系统调用时，仅仅把数据从内核缓冲区复制到应用的缓冲区（进程缓冲区）；上层应用使用write系统调用时，仅仅把数据从应用的缓冲区复制到内核缓冲区。在大多数情况下，Linux系统中用户程序的IO读写程序并没有进行实际的IO操作，而是在用户缓冲区和内核缓冲区之间直接进行数据的交换。

<img src="https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210605150414935.png" alt="image-20210605150414935" style="zoom: 67%;" />

一个完整输入流程的两个阶段：
•应用程序等待数据准备好：第一个阶段，应用程序等待数据通过网络到达网卡，当所等待的分组到达时，数据被操作系统复制到内核缓冲区中。这个工作由操作系统自动完成，用户程序无感知。
•从内核缓冲区向用户缓冲区复制数据：第二个阶段，内核将数据从内核缓冲区复制到应用的用户缓冲区。

### IO多路复用模型

select/epoll系统调用。通过该系统调用，一个用户进程（或者线程）可以监视多个文件描述符，一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核就能够将文件描述符的就绪状态返回给用户进程（或者线程），用户空间可以根据文件描述符的就绪状态进行相应的IO系统调用。
IO多路复用（IO Multiplexing）属于一种经典的Reactor模式实现，有时也称为异步阻塞IO，Java中的Selector属于这种模型。

在IO多路复用模型中通过select/epoll系统调用，单个应用程序的线程可以不断地轮询成百上千的socket连接的就绪状态，当某个或者某些socket网络连接有IO就绪状态时就返回这些就绪的状态（或者说就绪事件）。

（1）选择器注册。首先，将需要read操作的目标文件描述符（socket连接）提前注册到Linux的select/epoll选择器中，在Java中所对应的选择器类是Selector类。然后，开启整个IO多路复用模型的轮询流程。

（2）就绪状态的轮询。通过选择器的查询方法，查询所有提前注册过的目标文件描述符（socket连接）的IO就绪状态。通过查询的系统调用，内核会返回一个就绪的socket列表。当任何一个注册过的socket中的数据准备好或者就绪了就说明内核缓冲区有数据了，内核将该socket加入就绪的列表中，并且返回就绪事件。

（3）用户线程获得了就绪状态的列表后，根据其中的socket连接发起read系统调用，用户线程阻塞。内核开始复制数据，将数据从内核缓冲区复制到用户缓冲区。

（4）复制完成后，内核返回结果，用户线程才会解除阻塞的状态，用户线程读取到了数据，继续执行。

![image-20210605151630414](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210605151630414.png)

通过JDK的源码可以看出，Java语言的NIO组件在Linux系统上是使用epoll系统调用实现的。所以，Java语言的NIO组件所使用的就是IO多路复用模型。

IO多路复用模型的缺点是，本质上select/epoll系统调用是阻塞式的，属于同步IO，需要在读写事件就绪后由系统调用本身负责读写，也就是说这个读写过程是阻塞的。要彻底地解除线程的阻塞，就必须使用异步IO模型。



### 介绍

![img](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/f1c9e5d2b37d489e959ccd2d7573a713~tplv-k3u1fbpfcp-zoom-1.image)

non-blocking I/O或New IO

同步非阻塞，服务端可以开启一个线程处理多个连接，它是非阻塞的，客户端发送的数据都会注册到多路复用器selector上面，当selector（selector的select方法是阻塞的）轮询到有读、写或者连接请求时，才会转发到后端程序进行处理，没有数据的时候，业务程序并不需要阻塞等待。

![image-20210603090056517](https://tongji2021.oss-cn-shanghai.aliyuncs.com/img/image-20210603090056517.png)

### Channel

+ 类比于IO流，但是具有双向性，既可读，又可写
+ ServerSocketChannel用来监听客户端连接请求并创建SocketChannel与客户端进行通信
+ 只能通过Buffer读写Channel中数据

### Buffer

本质上是一块内存区域

**字段**

```java
private int mark = -1; // 存储特定position位置，后续通过reset恢复到该位置，依然可以读取这里的数据
private int position = 0; // 读写操作时的索引下标，最大为capacity-1;写模式切换到读模式时，会变为0
private int limit; // 写模式下等于capacity，切换到读模式时，limit表示最多能从buffer中读取多少，等于写模式下的position
private int capacity; // 容量，标识最大能容纳多少字节
```

**API实例**

```java
ByteBuffer byteBuffer = ByteBuffer.allocate(10); // position = 0,limit = 10,capacity=10
byteBuffer.put("aaa".getBytes()); // position = 3, limit = 10, capacity = 10
byteBuffer.flip(); // 从写模式切换到读模式 position = 0, limit = 3, capacity = 10
byteBuffer.get();  // position = 1, limit = 3, capacity = 10
byteBuffer.mark(); // mark = 1, position = 1, limit = 3, capacity = 10
byteBuffer.get();  // mark = 1, position = 2, limit = 3, capacity = 10
byteBuffer.reset();  // mark = 1, position = 1, limit = 3, capacity = 10
byteBuffer.clear(); // 所有属性重置 position = 0, limit = 10, capacity = 10
```

使用Java NIO Buffer类的基本步骤如下：
（1）使用创建子类实例对象的allocate()方法创建一个Buffer类的实例对象。
（2）调用put()方法将数据写入缓冲区中。
（3）写入完成后，在开始读取数据前调用Buffer.flip()方法，将缓冲区转换为读模式。
（4）调用get()方法，可以从缓冲区中读取数据。
（5）读取完成后，调用Buffer.clear()方法或Buffer.compact()方法，将缓冲区转换为写模式，可以继续写入。

### Selector

**选择器** /**多路复用器**。用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个channels，也就是可以管理多个网络连接。

### 示例程序

**NIOServer**

```java
package io;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.*;
import java.nio.charset.Charset;
import java.util.Iterator;
import java.util.Set;

public class NIOServer {
    public void start() throws IOException {
        Selector selector = Selector.open();
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        serverSocketChannel.bind(new InetSocketAddress(8848));
        serverSocketChannel.configureBlocking(false);
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);
        while (true) {
            int readyChannels = selector.select(); // 获取可用channel数量
            if (readyChannels == 0) {
                continue;
            }
            Set<SelectionKey> selectionKeys = selector.selectedKeys();
            Iterator<SelectionKey> iterator = selectionKeys.iterator();
            while (iterator.hasNext()) {
                SelectionKey selectionKey = iterator.next();
                iterator.remove();
                if (selectionKey.isReadable()) {
                    readHandler(selectionKey, selector);
                } else if (selectionKey.isAcceptable()) {
                    acceptHandler(serverSocketChannel, selector);
                }
            }
        }
    }

    private void acceptHandler(ServerSocketChannel serverSocketChannel, 
                               Selector selector) throws IOException {
        SocketChannel socketChannel = serverSocketChannel.accept();
        socketChannel.configureBlocking(false);
        socketChannel.register(selector, SelectionKey.OP_READ);
        // encode返回的是ByteBuffer
        socketChannel.write(Charset.defaultCharset().encode("成功连上了服务器!"));
    }

    private void readHandler(SelectionKey selectionKey, 
                             Selector selector) throws IOException {
        SocketChannel socketChannel = (SocketChannel) selectionKey.channel();
        ByteBuffer byteBuffer = ByteBuffer.allocate(1024);
        // 读取客户端数据
        StringBuilder msg = new StringBuilder();
        while (socketChannel.read(byteBuffer) > 0) {
            byteBuffer.flip();
            msg.append(Charset.defaultCharset().decode(byteBuffer));
        }
        // 将channel再次注册到selector上,监听它的可读事件
        socketChannel.register(selector, SelectionKey.OP_READ);
        if (msg.length() > 0) {
            System.out.println(msg.toString());
            broadCast(selector, socketChannel, msg.toString());
        }
    }

    private void broadCast(Selector selector, SocketChannel sourceChannel, String msg) {
        // 获取所有已接入客户端channel
        Set<SelectionKey> selectionKeys = selector.keys();
        selectionKeys.forEach(selectionKey -> {
            Channel targetChannel = selectionKey.channel();
            // 剔除发消息的那个channel
            if (targetChannel instanceof SocketChannel 
                && targetChannel != sourceChannel) {
                try {
                    // 向所有channel广播信息
                    ((SocketChannel) targetChannel).write(Charset.defaultCharset().encode(msg));
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        });
    }

    public static void main(String[] args) throws IOException {
        new NIOServer().start();
    }
}
```

**NIOClient**

```java
package io;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.SelectionKey;
import java.nio.channels.Selector;
import java.nio.channels.SocketChannel;
import java.nio.charset.Charset;
import java.util.Iterator;
import java.util.Scanner;
import java.util.Set;

public class NIOClient {
    static class NIOClientHandler implements Runnable {
        private final Selector selector;

        public NIOClientHandler(Selector selector) {
            this.selector = selector;
        }

        @Override
        public void run() {
            try {
                while (true) {
                    int readyChannels = selector.select(); // 获取可用channel数量
                    if (readyChannels == 0) {
                        continue;
                    }
                    Set<SelectionKey> selectionKeys = selector.selectedKeys();
                    Iterator<SelectionKey> iterator = selectionKeys.iterator();
                    while (iterator.hasNext()) {
                        SelectionKey selectionKey = iterator.next();
                        iterator.remove();
                        if (selectionKey.isReadable()) {
                            readHandler(selectionKey, selector);
                        }
                    }
                }
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        private void readHandler(SelectionKey selectionKey, 
                                 Selector selector) throws IOException {
            SocketChannel socketChannel = (SocketChannel) selectionKey.channel();
            ByteBuffer byteBuffer = ByteBuffer.allocate(1024);
            // 读取服务器端响应数据
            StringBuilder msg = new StringBuilder();
            while (socketChannel.read(byteBuffer) > 0) {
                byteBuffer.flip();
                msg.append(Charset.defaultCharset().decode(byteBuffer));
            }
            // 将channel再次注册到selector上,监听它的可读事件
            socketChannel.register(selector, SelectionKey.OP_READ);
            if (msg.length() > 0) {
                System.out.println(msg.toString());
            }
        }
    }

    public void start() throws IOException {
        SocketChannel socketChannel = 
            SocketChannel.open(new InetSocketAddress("127.0.0.1", 8848));
        // 新开线程，专门接收服务器端发送的信息
        Selector selector = Selector.open();
        socketChannel.configureBlocking(false);
        socketChannel.register(selector, SelectionKey.OP_READ);
        new Thread(new NIOClientHandler(selector)).start();

        Scanner scanner = new Scanner(System.in);
        while (scanner.hasNextLine()) {
            String msg = scanner.nextLine();
            if (msg != null && msg.length() > 0) {
                socketChannel.write(Charset.defaultCharset().encode(msg));
            }
        }
    }

    public static void main(String[] args) throws IOException {
        new NIOClient().start();
    }
}
```



## AIO

Asynchronous I/O或NIO.2

异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理

**AIO**提供了从建立连接到读、写的全异步操作。**AIO**可用于异步的**文件读写**和**网络通信**。

**Server**

```java
package io;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.AsynchronousServerSocketChannel;
import java.nio.channels.AsynchronousSocketChannel;
import java.nio.channels.CompletionHandler;
import java.nio.charset.Charset;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

public class AIOServer {
    public static void main(String[] args) {
        try {
            final int port = 8848;
            AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open();
            serverSocketChannel.bind(new InetSocketAddress(port));
            // 需要在handler的实现中处理连接请求和监听下一个连接、数据收发，以及通信异常。
            // 消息处理回调接口，是一个负责消费异步IO操作结果的消息处理器
            CompletionHandler<AsynchronousSocketChannel, Object> handler = new CompletionHandler<AsynchronousSocketChannel, Object>() {
                /*
                当I/O操作成功完成时，会回调到completed方法，failed方法则在I/O操作失败时被回调。
                需要注意的是：在CompletionHandler的实现中应当及时处理操作结果，以避免一直占用调用线程而不能分发其他的CompletionHandler处理器。
                 */
                @Override
                public void completed(AsynchronousSocketChannel result, Object attachment) {
                    // 继续监听下一个连接请求
                    serverSocketChannel.accept(attachment, this);
                    try {
                        System.out.println("接受了一个连接：" + result.getRemoteAddress().toString());
                        // result表示当前接受的客户端的连接会话，与客户端的通信都需要通过该连接会话进行。
                        result.write(Charset.defaultCharset().encode("Server:Hello World"));

                        ByteBuffer readBuffer = ByteBuffer.allocate(128);
                        result.read(readBuffer).get();
                        System.out.println(new String(readBuffer.array()));
                    } catch (IOException | InterruptedException | ExecutionException e) {
                        e.printStackTrace();
                    }
                }

                @Override
                public void failed(Throwable exc, Object attachment) {
                    System.out.println("出错了：" + exc.getMessage());
                }
            };
            // 是一个异步方法，调用会直接返回,为了让子线程能够有时间处理监听客户端的连接会话，这里让主线程休眠一段时间
            serverSocketChannel.accept(null, handler);
            TimeUnit.MINUTES.sleep(Integer.MAX_VALUE);
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
    }
}
```

**Client**

```java
package io;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.AsynchronousSocketChannel;
import java.nio.channels.CompletionHandler;
import java.nio.charset.Charset;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

public class AIOClient {
    public static void main(String[] args) {
        try {
            AsynchronousSocketChannel client = AsynchronousSocketChannel.open();
            client.connect(new InetSocketAddress("127.0.0.1", 8848), null, new CompletionHandler<Void, Object>() {
                @Override
                public void completed(Void result, Object attachment) {
                    System.out.println("成功连接到服务器!");
                    try {
                        client.write(Charset.defaultCharset().encode("Client:Hello World"));
                        ByteBuffer readBuffer = ByteBuffer.allocate(128);
                        client.read(readBuffer).get();
                        System.out.println(new String(readBuffer.array()));
                    } catch (InterruptedException | ExecutionException e) {
                        e.printStackTrace();
                    }
                }

                @Override
                public void failed(Throwable exc, Object attachment) {
                    exc.printStackTrace();
                }
            });
            TimeUnit.MINUTES.sleep(Integer.MAX_VALUE);
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
    }
}
```

